# Where Are the AI-Native Engineers?

So I posted a job opening a few weeks ago. AI-native engineer at [Cambryo](https://cambryo.com). Sexy title, I know. The response was massive. Hundreds of applications. And almost nobody who actually fits.

That's not a knock on the people who applied. Most of them are talented, some genuinely brilliant. But "AI-native engineer" is one of those titles that sounds like it means one thing and actually means something quite different.

Here's what I mean. The person I'm looking for is full-stack comfortable, close to the customer, opinionated about architecture and devops, but doesn't spend their days staring at source code. They orchestrate agents. Claude Code, Cursor, whatever comes next (and something always comes next). They automate away the routine work. They review and approve PRs rather than writing every line themselves. The role is less "craftsperson at a workbench" and more "conductor who knows every instrument." Still deeply technical, but the work is orchestration, not coding.

I'm also involved with a group of applied IT bachelor students, and if anyone should be ready for this role, it's them. They're young, digital-native, enthusiastic. But here's the thing: orchestrating AI agents requires the kind of architectural instinct and domain sense that usually comes from years of experience. You need to know what good looks like before you can evaluate whether an agent produced something good. It's a bit like graduating and immediately being asked to manage a team of ten. You might have the raw talent, but the pattern recognition isn't there yet. (And I say this with genuine sympathy, not as some grumpy old guy shaking his fist, even if I am known to sometimes do this.)

I don't think education is going to fix this anytime soon either. The tools change every six months. What students learn in their first year is outdated by their third. Keeping curricula current is expensive, license costs for these tools are non-trivial, and universities, by design, move slowly. There's a cultural mismatch too. CS education has always built identity around crafting elegant code, around the beauty of a well-structured algorithm. Taking time to research properly, following a clear roadmap, thinking before doing. And those are genuinely good things. It's just that this new world asks for something different. Not necessarily better or worse, just different.

And here's where I guess I have a bit of a hot take. As a company, as a team, the goal was always to build something valuable for your customer. But as an individual engineer, you could afford to just care about a clean codebase, elegant abstractions, the craft of it. Someone else worried about whether the customer actually got what they needed. That's changing. The whole organizational translation layer that we built up over decades, PMs translating business needs, analysts translating data, engineers translating specs into software, that's collapsing. The engineer is becoming the product thinker. You're not getting a requirements doc from someone three meetings removed from the actual user. You're sitting with the customer, understanding their problem, and shipping a solution. The code is almost incidental now.

I realize that sounds dramatic. And I'm not sure I've fully thought it through. But when I look at the best work happening in AI-native teams, the engineers who thrive are the ones who care more about the outcome than the implementation. They're pragmatic, a little impatient with ceremony, and obsessed with whether the thing they built actually matters to someone.

The problem is I can't find enough of them. (I also can't fully articulate what makes someone "enough of one," which is its own issue.) I'm still looking. Still figuring out what the right interview even looks like for this kind of role. And honestly, still wondering whether I'm describing a real job or just a fantasy of what engineering could become.

I guess if you're reading this and thinking "that sounds like me," you should probably reach out. And if you're reading this and thinking "this guy has no idea what he's talking about," well. You might even be right.
